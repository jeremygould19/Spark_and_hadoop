Collection of projects I've worked on using Hadoop and the Hadoop distributed file system. These projects come from working through the Coursera course, Big Data Essentials: HDFS, MapReduce, and Spark RDD.  See more information on the
course at the following link:
https://www.coursera.org/learn/big-data-essentials

Goals of the course were to:
- learn basic technologies of the modern Big Data landscape, namely: HDFS, MapReduce and Spark;
- be guided through systems internals and their applications;
- learn about distributed file systems, why they exist and what function they serve;
- grasp the MapReduce framework, a workhorse for many modern Big Data applications;
- apply the framework to process texts and solve sample business cases;
- learn about Spark, the next-generation computational framework;
- build a strong understanding of Spark basic concepts;
- develop skills to apply these tools to creating solutions in finance, social networks, telecommunications and many other fields.
